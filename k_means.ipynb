{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the csv file\n",
    "df=pd.read_csv('netflix_titles.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the null values with mode for the numeric column and then the columns with string is filled with NULL\n",
    "df['country'] = df['country'].fillna(df['country'].mode()[0])\n",
    "df['date_added'] = df['date_added'].fillna(df['date_added'].mode()[0])\n",
    "df['rating'] = df['rating'].fillna(df['country'].mode()[0])\n",
    "df['duration'] = df['duration'].fillna('0 min')\n",
    "df['cast'] = df['cast'].fillna('NULL')\n",
    "df['director'] = df['director'].fillna('NULL')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_id         0\n",
       "type            0\n",
       "title           0\n",
       "director        0\n",
       "cast            0\n",
       "country         0\n",
       "date_added      0\n",
       "release_year    0\n",
       "rating          0\n",
       "duration        0\n",
       "listed_in       0\n",
       "description     0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking is any Missing data in each column of the dataset.\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mini-batch K-means clustering algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Mini-batch K-means clustering algorithm is a version of the standard K-means algorithm in machine learning. It uses small, random, fixed-size batches of data to store in memory, and then with each iteration, a random sample of the data is collected and used to update the clusters.\n",
    "\n",
    "Each iteration a new random sample from the dataset is obtained and used to update the clusters and this is repeated until convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1=df\n",
    "df1[\"date_added\"] = pd.to_datetime(df['date_added'])\n",
    "df1['directors'] = df['director'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "df1['categories'] = df['listed_in'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "df1['actors'] = df['cast'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])\n",
    "df1['countries'] = df['country'].apply(lambda l: [] if pd.isna(l) else [i.strip() for i in l.split(\",\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "#Define a TF-IDF Vectorizer Object. Remove all english stop words such as 'the', 'a'\n",
    "tfidf = TfidfVectorizer(stop_words='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8807, 18895)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Construct the required TF-IDF matrix by fitting and transforming the data\n",
    "tfidf_matrix = tfidf.fit_transform(df['description'])\n",
    "\n",
    "#Output the shape of tfidf_matrix\n",
    "tfidf_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import linear_kernel\n",
    "from sklearn.metrics.pairwise import linear_kernel\n",
    "\n",
    "# Compute the cosine similarity matrix\n",
    "cosine_sim = linear_kernel(tfidf_matrix, tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_similar(tfidf_matrix, index, top_n = 5):\n",
    "    cosine_similarities = linear_kernel(tfidf_matrix[index:index+1], tfidf_matrix).flatten()\n",
    "    related_docs_indices = [i for i in cosine_similarities.argsort()[::-1] if i != index]\n",
    "    return [index for index in related_docs_indices][0:top_n] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the tfidf matrix with the descriptions\n",
    "import time\n",
    "start_time = time.time()\n",
    "text_content = df1['description']\n",
    "vector = TfidfVectorizer(max_df=0.4,         # drop words that occur in more than X percent of documents\n",
    "                             min_df=1,      # only use words that appear at least X times\n",
    "                             stop_words='english', # remove stop words\n",
    "                             lowercase=True, # Convert everything to lower case \n",
    "                             use_idf=True,   # Use idf\n",
    "                             norm=u'l2',     # Normalization\n",
    "                             smooth_idf=True # Prevents divide-by-zero errors\n",
    "                            )\n",
    "tfidf = vector.fit_transform(text_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ayush\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "190    7601\n",
       "137     521\n",
       "26      211\n",
       "75      168\n",
       "115     109\n",
       "Name: cluster, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans# Clustering  Kmeans\n",
    "k = 200\n",
    "kmeans = MiniBatchKMeans(n_clusters = k)\n",
    "kmeans.fit(tfidf)\n",
    "centers = kmeans.cluster_centers_.argsort()[:,::-1]\n",
    "terms = vector.get_feature_names()\n",
    "request_transform = vector.transform(df1['description'])\n",
    "# new column cluster based on the description\n",
    "df1['cluster'] = kmeans.predict(request_transform) \n",
    "df1['cluster'].value_counts().head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " iter 0 -- 0.04700303077697754 seconds --\n",
      " iter 1000 -- 7.165312767028809 seconds --\n",
      " iter 2000 -- 13.336365699768066 seconds --\n",
      " iter 3000 -- 21.201383352279663 seconds --\n",
      " iter 4000 -- 27.7522075176239 seconds --\n",
      " iter 5000 -- 33.37062931060791 seconds --\n",
      " iter 6000 -- 39.01405191421509 seconds --\n",
      " iter 7000 -- 44.622472524642944 seconds --\n",
      " iter 8000 -- 50.33790040016174 seconds --\n",
      " finish -- 54.98725199699402 seconds --\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "P = nx.Graph(label=\"MOVIE\")\n",
    "start_time = time.time()\n",
    "for i, rowi in df1.iterrows():\n",
    "    if (i%1000==0):\n",
    "        print(\" iter {} -- {} seconds --\".format(i,time.time() - start_time))\n",
    "    P.add_node(rowi['title'],key=rowi['show_id'],label=\"MOVIE\",mtype=rowi['type'],rating=rowi['rating'])\n",
    "    P.add_node(rowi['cluster'],label=\"CLUSTER\")\n",
    "    P.add_edge(rowi['title'], rowi['cluster'], label=\"DESCRIPTION\")\n",
    "    for element in rowi['actors']:\n",
    "        P.add_node(element,label=\"PERSON\")\n",
    "        P.add_edge(rowi['title'], element, label=\"ACTED_IN\")\n",
    "    for element in rowi['categories']:\n",
    "        P.add_node(element,label=\"CAT\")\n",
    "        P.add_edge(rowi['title'], element, label=\"CAT_IN\")\n",
    "    for element in rowi['directors']:\n",
    "        P.add_node(element,label=\"PERSON\")\n",
    "        P.add_edge(rowi['title'], element, label=\"DIRECTED\")\n",
    "    for element in rowi['countries']:\n",
    "        P.add_node(element,label=\"COU\")\n",
    "        P.add_edge(rowi['title'], element, label=\"COU_IN\")\n",
    "    indices = find_similar(tfidf, i, top_n = 5)\n",
    "    snode=\"Sim(\"+rowi['title'][:15].strip()+\")\"        \n",
    "    P.add_node(snode,label=\"SIMILAR\")\n",
    "    P.add_edge(rowi['title'], snode, label=\"SIMILARITY\")\n",
    "    for element in indices:\n",
    "        P.add_edge(snode, df1['title'].loc[element], label=\"SIMILARITY\")\n",
    "print(\" finish -- {} seconds --\".format(time.time() - start_time))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def get_recommendation(root):\n",
    "    commons_dict = {}\n",
    "    for h in P.neighbors(root):\n",
    "        for h2 in P.neighbors(h):\n",
    "            if h2==root:\n",
    "                continue\n",
    "            if P.nodes[h2]['label']==\"MOVIE\":\n",
    "                commons = commons_dict.get(h2)\n",
    "                if commons==None:\n",
    "                    commons_dict.update({h2 : [h]})\n",
    "                else:\n",
    "                    commons.append(h)\n",
    "                    commons_dict.update({h2 : commons})\n",
    "    movies=[]\n",
    "    weight=[]\n",
    "    for key, values in commons_dict.items():\n",
    "        w=0.0\n",
    "        for h in values:\n",
    "            w=w+1/math.log(P.degree(h))\n",
    "        movies.append(key) \n",
    "        weight.append(w)\n",
    "    \n",
    "    final = pd.Series(data=np.array(weight),index=movies)\n",
    "    final.sort_values(inplace=True,ascending=False)        \n",
    "    return final;\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****************************************\n",
      " Recommendation for 'PK\n",
      "****************************************\n",
      "3 Idiots                 2.107430\n",
      "Sanju                    1.722554\n",
      "Merku Thodarchi Malai    1.625123\n",
      "Drive                    1.594257\n",
      "Chance Pe Dance          1.553858\n",
      "dtype: float64\n",
      "****************************************\n",
      " Recommendation for 'Ocean's Thirteen'\n",
      "****************************************\n",
      "Ocean's Twelve       6.207969\n",
      "The Departed         2.210179\n",
      "Ocean's Eleven       2.075885\n",
      "Hostel: Part III     1.793146\n",
      "Brooklyn's Finest    1.446086\n",
      "dtype: float64\n",
      "****************************************\n",
      " Recommendation for 'Stranger Things'\n",
      "****************************************\n",
      "Beyond Stranger Things     11.419323\n",
      "Rowdy Rathore               2.688934\n",
      "Safe Haven                  2.461772\n",
      "Big Stone Gap               1.903662\n",
      "The Autopsy of Jane Doe     1.903662\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "final1 = get_recommendation(\"PK\")\n",
    "final2 = get_recommendation(\"Ocean's Thirteen\")\n",
    "final3 = get_recommendation(\"The Devil Inside\")\n",
    "final4 = get_recommendation(\"Stranger Things\")\n",
    "print(\"*\"*40+\"\\n Recommendation for 'PK\\n\"+\"*\"*40)\n",
    "print(final1.head())\n",
    "print(\"*\"*40+\"\\n Recommendation for 'Ocean's Thirteen'\\n\"+\"*\"*40)\n",
    "print(final2.head())\n",
    "\n",
    "print(\"*\"*40+\"\\n Recommendation for 'Stranger Things'\\n\"+\"*\"*40)\n",
    "print(final4.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0467c16d4e73dba61dcc106221b4b7c95372fa61fc6cc648ee99bca30bbaf279"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
